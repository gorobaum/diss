%% ------------------------------------------------------------------------- %%
\chapter{Metodologia}
\label{cap:metodologia}

\section{Thin Plate Splines}\index{TPS}\label{thinPlateSplines}

  O \textit{Thin plate splines} (TPS) \cite{bookstein1989principal}
\cite{goshtasby1988registration} é um algoritmo de registro não rígido,
que aproxima o processo de registro à deformação de uma fina placa de metal.
A placa é deformada pela força aplicada por cada uma das características encontradas
nas imagens e pelas forças de interação entre cada par de características. Dado
seus resultados, e relativa eficiência, é um dos algoritmos mais aplicados em
registro de imagens médicas, por \cite{goshtasby2005} e \cite{rohr1999approximating}.

  A função de deformação adotada pelo TPS é uma função de interpolação radial. A
função modificada para registro de imagens tridimensionais é:
\begin{align}\label{math:tps}
  \begin{split}
    f(x, y, z) &= A_0 + A_1x + A_2y + A_3z + \sum_{i=0}^n F_i r_i^2 ln (r_i^2) \\
    r_i^2 &= (x-x_i)^2 + (y-y_i)^2 + (z-z_i)^2 + d^2
  \end{split}
\end{align}

  Onde $d$ é um fator de rigidez de superfície, utilizado para diminuir a ação
das características sobre a placa de metal e $x, y, z$ são as coordenadas das
características. A função do TPS é capaz de representar deformações
rígidas e não rígidas. Deformações rígidas são calculadas pela primeira parte
da função, $A_0 + A_1x + A_2y + A_3z$, enquanto deformações não rígidas são
calculadas por $\sum_{i=0}^n F_i r_i^2 ln (r_i^2)$.

  Para definir os valores das variáveis $A_0, A_1, A_2, A_3, F_i, \{i = 0, ..., n\}$,
o seguinte sistema linear é utilizado:

\begin{align}\label{math:tpsSystem}
\begin{split}
    \sum_{i=1}^n F_i &= 0 \\
    \sum_{i=1}^n F_ix &= 0 \\
    \sum_{i=1}^n F_iy &= 0 \\
    \sum_{i=1}^n F_iz &= 0 \\
    f(x_1,y_1,z_1) &= A_0 + A_1x + A_2y + A_3z + \sum_{i=0}^n F_i r_{i1}^2 ln r_{i1}^2 \\
    \vdots \\
    f(x_n,y_n,z_n) &= A_0 + A_1x + A_2y + A_3z + \sum_{i=0}^n F_i r_{in}^2 ln r_{in}^2
\end{split}
\end{align}

  As quatro primeiras equações aplicam restrições de movimento sobre a
superfície na qual a transformação é aplicada. A primeira equação,
$\sum_{i=1}^n F_i = 0$, garante que a soma dos pesos aplicados à superfície
seja zero, logo o superfície não sofre translações. As três próximas equações,
$\sum_{i=1}^n F_ix = 0$, $\sum_{i=1}^n F_iy = 0$ e $\sum_{i=1}^n F_iz = 0$,
garantem que o momento aplicado em cada uma das três dimensões seja zero, e
assim que a superfície não rotacione em relação a nenhum dos eixos.

\subsubsection{Pseudo Algoritmo e Tempo Assintótico de Execução}

  O TPS recebe como entrada a correspondência entre os pontos de controle, a
imagem alvo e a imagem referência. O primeiro passo do TPS é resolver o
sistema linear construído a partir da equação \ref{math:tpsSystem}. Esse sistema
é subdividido em três sistemas distintos, cada um aplicado à uma das dimensões
da imagem. Com o resultado dos sistemas lineares, o TPS tem os valores
necessários para computar o próximo passo, dado pela equação \ref{math:tps}.
A imagem resultado é construída por essa equação, que recebe as coordenadas da
imagem resultado e devolve a intensidade do pixel construída a partir de uma
interpolação de pixels da imagem alvo.

  Os dois blocos de pseudo código, \ref{code:tps} e \ref{code:alinhamento}, representam
a implementação realizada nesse trabalho. A função \codepagode{tps} orquestra toda a
execução. Ela recebe as entradas, chama a função \codepagode{resolverEquLinear} que
resolve cada um dos três sistemas lineares, passa seus resultados para a função
\codepagode{alinhamento}, que alinha as imagens, por fim calculando o valor dos
voxels na imagem resultado usando a função \codepagode{interpolar}.


\IncMargin{1em}
\begin{algorithm}
    \SetKwInput{Input}{Input}
    \SetKwInput{Output}{Output}

    \underline{function tps} (imgRef, imgAlvo, caracRef, caracAlvo)\;
    \Input{A imagem referencia, imagem alvo, conjunto de características da imagem referencia, conjunto de características da imagem alvo}
    \Output{A imagem resultado}
    \emph{solucaoEquLinX $\leftarrow$ resolverEquLinear(caracRef.x, caracAlvo.x)} \;
    \emph{solucaoEquLinY $\leftarrow$ resolverEquLinear(caracRef.y, caracAlvo.y)} \;
    \emph{solucaoEquLinZ $\leftarrow$ resolverEquLinear(caracRef.z, caracAlvo.z)} \;

    \For{$x \leftarrow 0$ \KwTo $n_c$} {
      \For{$y \leftarrow 0$ \KwTo $n_l$} {
        \For{$z \leftarrow 0$ \KwTo $n_p$} {
            \emph{novoX $\leftarrow$ alinhamento(x, y, z, solucaoEquLinX, caracRef)} \;
            \emph{novoY $\leftarrow$ alinhamento(x, y, z, solucaoEquLinY, caracRef)} \;
            \emph{novoZ $\leftarrow$ alinhamento(x, y, z, solucaoEquLinZ, caracRef)} \;
            \emph{novaImg[x, y, z] $\leftarrow$ interpolar(novoX, novoY, novoZ, imgAlvo)} \;
        }
      }
    }
    \caption{Pseudo código da função tps.}\label{code:tps}
\end{algorithm}\DecMargin{1em}

\IncMargin{1em}
\begin{algorithm}
    \SetKwInput{Input}{Input}
    \SetKwInput{Output}{Output}

    \underline{function alinhamento} (x, y, z, solucaoEquLin, caracRef)\;
    \Input{As coordenadas do ponto na imagem referencia, a solução da equação linear em uma determinada dimensão
           e o conjunto de características da imagem referencia}
    \Output{A nova coordenada do ponto na imagem alvo em uma determinada dimensão}

    \emph{v$[0] \leftarrow solucaoEquLin[0]$} \;
    \emph{v$[1] \leftarrow x * solucaoEquLin[1]$} \;
    \emph{v$[2] \leftarrow y * solucaoEquLin[2]$} \;
    \emph{v$[3] \leftarrow z * solucaoEquLin[3]$} \;

    \For{$n \leftarrow 0$ \KwTo $n_{cp}$} {
        \emph{$x_i \leftarrow caracRef[n].x$} \;
        \emph{$y_i \leftarrow caracRef[n].y$} \;
        \emph{$z_i \leftarrow caracRef[n].z$} \;
        \emph{$r \leftarrow (x-x_i)^2 + (y-y_i)^2 + (z-z_i)^2$} \;
        \emph{v$[n+4] \leftarrow r * \log{r} * solucaoEquLin[n+4]$} \;
    }
    \caption{Pseudo código da função alinhamento.}\label{code:alinhamento}
\end{algorithm}\DecMargin{1em}

  As tabelas abaixo mostram o tempo de execução assintótico do tps, totalizando
$\mathcal{O}(n_{cp}^3+n_l*n_c*n_{cp})$:

\begin{table}[H]
\begin{center}
\begin{tabular}{l|c}
\hline
Tempo de execução do algoritmo \textit{tps} \\
\hline
Função & Tempo\\
\hline
2       &$\mathcal{O}(n_{cp}^3)$\\
3       &$\mathcal{O}(n_{cp}^3)$\\
4       &$\mathcal{O}(n_{cp}^3)$\\
5       &$\mathcal{O}(n_c)$\\
6       &$\mathcal{O}(n_l)$\\
7       &$\mathcal{O}(n_p)$\\
8       &$\mathcal{O}(n_{cp})$\\
9       &$\mathcal{O}(n_{cp})$\\
10       &$\mathcal{O}(n_{cp})$\\
11       &$\mathcal{O}(1)$\\
\hline
Total   &$\mathcal{O}(n_{cp}^3+n_l \times n_c \times n_p \times n_{cp})$\\
\hline
\end{tabular}
\caption{Tempo de execução do algoritmo \textit{tps}}
\label{table:tps}
\end{center}
\end{table}

  O tempo de execução da resolução do sistema linear foi retirada do livro escrito por
\cite[Part~IV]{trefethen1997numerical}, assumindo uma fatoração LU da matriz. O tempo de execução da interpolação assume uma interpolação trilinear.

\begin{table}[H]
\begin{center}
\begin{tabular}{l|c}
\hline
Tempo de execução do algoritmo \textit{alinhamento} \\
\hline
Linha&Tempo\\
\hline
2       &$\mathcal{O}(1)$ \\
3       &$\mathcal{O}(1)$ \\
4       &$\mathcal{O}(1)$\\
5       &$\mathcal{O}(1)$\\
6       &$\mathcal{O}(n_{cp})$\\
7       &$\mathcal{O}(1)$\\
8       &$\mathcal{O}(1)$\\
9       &$\mathcal{O}(1)$\\
10      &$\mathcal{O}(1)$\\
11      &$\mathcal{O}(1)$\\
\hline
Total   &$\mathcal{O}(n_{cp})$\\
\hline
\end{tabular}
\caption{Tempo de execução do algoritmo \textit{interpolar}}
\label{table:interpolar}
\end{center}
\end{table}

%% ------------------------------------------------------------------------- %%
\section{Implementação do Thin Plate Splines em GPU}

\subsection{Mapeamento da resolução do sistema linear}

  Pela natureza da sua construção, os sistemas lineares montados pelos TPS dificilmente
serão esparsos, logo algoritmos mais simples podem ser aplicados na sua solução
sem perda de desempenho. Essa etapa do mapeamento foi resolvida escolhendo a
biblioteca \textit{cuSolver}, da \cite{cuSolver}. A fatoração QR foi escolhida
para resolver o sistema linear.

  Os sistemas lineares tem tamanho $n_{cp}^2$, e a solução tem tamanho $n_{cp}$.
Logo, é necessário transferir $(n_{cp}^2 + n_{cp}) \times 3 \times 8$ bytes de
dados para a GPU. O \textit{cuSolver} utiliza a matriz com o sistema linear para
armazenar os resultados dos passos intermediários, deminuindo o consumo de memória
do algoritmo.

\subsection{Mapeamento da função de deformação e interpolação}\label{segundoPasso}

  O segundo passo do TPS envolve a função de deformação e a interpolação dos
resultados da mesma. Como entrada são recebidos os resultados do passo anterior,
os voxels da imagem alvo e a lista das características. O resultado final é gravado
em memória da GPU, que é copiado para a memória da CPU após a execução do kernel.

  O passo foi implementado em um kernel CUDA na linguagem C. Cada thread do Kernel
é mapeada para o cálculo da função de deformação de um voxel da imagem, logo são necessários
$\left \lceil{\frac{n_l}{8}}\right \rceil \times
\left \lceil{\frac{n_c}{8}}\right \rceil \times
\left \lceil{\frac{n_p}{8}}\right \rceil$ blocos, respectivamente para
cada dimensão da image, e cada bloco tem dimensão $8 \times 8 \times 8$.

  Somando o espaço em memória dos parametros com o da imagem resultado, o kernel
reserva $N \times 2 \times 1 + n_{cp} \times 6 \times 8$ bytes

  Com os resultados em mãos, o segundo passo é calcular a função de deformação.
A função foi implementada em um kernel CUDA. É necessário prover os voxels da
imagem alvo, as soluções dos três sistemas lineares, a lista de pontos de controle
e o espaço em memória para gravar a imagem resultado. São necessários, então,
$(N \times 2 + n_{cp} \times 6) \times 8$ bytes. O código do kernel está em \ref{code:tpsBasic}.

  As linhas 6, 7 e 8 usam o ID do bloco e ID da thread para calcular o voxel que será
gerado. As linhas 10, 11 e 12 calculam a deformação rígida. O laço da linha 14
calcula da deformação não rígida. Por fim, uma interpolação trilinear é aplicada
para resgatar a intensidade do voxel da imagem alvo que é usada para gerar a imagem
resultado. A checagem antes da interpolação é necessária pois o número de threads
usadas pelo kernel é maior que o número de voxels da imagem, salvo pelas dimensões
que sejam múltiplas de 8.

%% ------------------------------------------------------------------------- %%
%% ------------------------------------------------------------------------- %%
\section{Melhorias de desempenho}\label{melhoriaDesempenho}

  O objetivo desse trabalho é apresentar uma implementação de alto desempenho
do algoritmo Thin plate splines em um ambiente GPGPU. O mapeamento apresentado
na seção anterior, por maior que seja seu aumento de velocidade comparativamente
com uma implementação em CPU, não utiliza nenhum método de melhoria de desempenho.
Essa seção se aprofunda em métodos aplicados para o aumento do desempenho de
algoritmos desenvolvidos em \textbf{CUDA}, enquanto avaliamos a aplicabilidade
de táis métodos ao TPS.

\subsection{Melhorias de desempenho associadas à memória}

O acesso a memória é o responsável por grande parte da queda de desempenho
em kernels. Isso se deve, em grande parte, pela diferença de comunicação entre o
\textit{Host} e a memória principal da GPU e a comunicação entre a memória
principal da GPU e os multiprocessadores
(8 GB/s contra 177.6 GB/s na comparação feita em \cite{cudaBestPractices}). Logo,
a transferência de memória entre \textit{Host} e \textit{Device} deve ser
minimizada. Para tanto, a implementação apresentada neste trabalho realiza uma
etapa de pré-processamento onde o máximo de memória é reservado na GPU.

A implementação é focada no registro de várias imagens alvo para uma única
imagem referência, logo os dados da imagem referência são transmitidos para a
GPU uma única vez. O TPS só utiliza a lista de características da imagem referência.
O próximo passo é ocupar o máximo possível de memória com instâncias diferentes
de registro. Cada instância de registro, como dito na seção \ref{segundoPasso},
precisa dos voxels da imagem alvo, das suas características e de espaço suficiente
para guardar a imagem resultado. Ainda para diminuir o tempo de transferência,
as soluções dos sistemas lineares são mantidos em memória na GPU.

O segundo método utilizado foi o mapeamento dos voxels da imagem alvo para uma
textura tridimensional. A memória de textura é preservada em cache, com privilégio
somente de leitura. Além disso, estão presentes no hardware as seguintes operações
em texturas: interpolação, normalização de coordenadas e checagem automática de
fronteira.

Por último, a fim de minimizar o \textit{downtime} dos multiprocessadores da GPU, a
execução paralela de várias instâncias de registro foi aplicada. O paralelismo
é iniciado na CPU, onde cada chamada do kernel do TPS parte de uma thread separada.
O escalonador da GPU é capaz de criar contextos diferentes para cada instância
diferente, e organizar os recursos da GPU entre todas as instâncias. A execução
em paralelo de vários kernels esconde a latência de cópia de memória entre a
CPU e GPU executando um kernel que já esteja pronto enquanto popula a memória
para a execução de outro.

No entanto, nem todas as técnicas de melhoria de desempenho são aplicáveis ao
TPS. O uso de \textit{Pinned memory}, método em que a memória da CPU é mapeada
diretamente a uma região da memória da CPU, aumentando a taxa de transferência entre
os diferentes espaços de memória. Mas esse mapeamento introduz uma restrição ao
kernel, já que leituras ou escritas feitas por threads diferentes ao mesmo banco
de memória causam lentidão. A memória compartilhada não foi explorada, já que
cada bloco divide entre todas as suas \textit{threads} uma pequena seção do
bloco de memória compartilhada, o que limita o que pode ser mantido nesta região.

\subsection{Melhorias de desempenho associadas ao processamento}

  Melhorar o desempenho do processamento de um kernel quer dizer maximizar o
tempo que cada um dos multiprocessadores está ocupando executando algo. Atingir
esse objetivo não é trivial, pois a distribuição das threads entre multiprocessadores
é parametrizada por vários fatores como o número disponível de registradores
em cada multiprocessador e a quantidade de memória compartilhada por bloco, por exemplo.
Como explicado na seção \ref{GPU}, o warp é a menor unidade de execução em um
multiprocessador. Cada warp é constituída de 32 \textit{threads}, que executam,
ao mesmo tempo, a mesma linha de código. A razão entre o número máximo de warps
em execução de um kernel pelo número máximo de warps em execução no hardware,
chamada de \textbf{Ocupação}, será a métrica usada para calcular o desempenho
de um kernel.

  A API do CUDA disponibiliza uma função que recebe o kernel e um tamanho de
bloco e retorna a ocupação da GPU dado esses parametros. Utilizando essa função,
uma calculadora de Ocupação foi construida. A calculadora incrementa o
tamanho do bloco de 32 em 32 threads (para garantir que um novo warp sempre
seja criado), e o bloco que obter a maior taxa de Ocupação é escolhido, com
preferência para blocos pequenos se houver algum empate. Essa preferência beneficia
a execução em paralelo de vários kernels.

  Outra técnica aplicada foi a de controlar o número de registradores do kernel.
Sejam $TM$ o número máximo de \textit{threads} por multiprocessador e $RB$ o
número máximo de registradores por bloco, o número máximo de registradores que
cada \textit{thread} pode usar é $\left \lceil{\frac{RB}{TM}}\right \rceil$.
O número de registradores utilizado por thread pode ser verificado utilizando
flags na compilação. Nas arquiteturas atuais, o número de registradores
utilizados por thread deve ficar abaixo de $32$. Esse número foi atingido pelos
kernels escritos para este trabalho.

  Maximizar o número de \textit{threads} executando em um dado momento ajuda em
esconder a latência das instruções das mesmas. Ao executar uma instrução,
como por exemplo um acesso a memória, todas as \textit{threads} de um warp
ficam bloqueadas esperando o resultado. Para compensar isso, o multiprocessador
pode executar instruções de um outro warp. Como definido em \cite{cudaProgrammingGuide},
seja $L$ a quantidade de ciclos do \textit{clock} necessário para executar uma
instrução, são necessárias $8L$ instruções para esconder sua latência, dado um
\textit{device} de \textit{compute capabillity} $3.x$.

  Por fim, a execução paralela de vários kernels, alem de esconder a latência
na cópia de memória como dito na seção acima, esconde a latência de instruções.
Com vários kernels executando ao mesmo tempo, o escalonador da GPU pode escolher
um warp associado a outro kernel enquanto espera a execução de outro warp.
