%% ------------------------------------------------------------------------- %%
\chapter{Conceitos}
\label{cap:conceitos}

%% ------------------------------------------------------------------------- %%
\section{Registro}\index{Registro}
\label{sec:fundamentos}

    O registro de imagens tem como objetivo encontrar um alinhamento entre duas imagens diferentes. Dada duas imagens, 
a imagem Referência (R) e a imagem Alvo (A), os algoritmos de registro encontram um campo
vetorial de deslocamento que é aplicado a imagem Alvo, movendo seus pixels para um estado no qual ela esteja alinhada
com a imagem Referência. Para encontrar um alinhamento, supomos que a imagem Alvo sofreu algum tipo de deformação,
e queremos, dado algum modelo que nos de informação sobre a deformação, encontrar os parametros para uma função de
transformação que nos leve da imagem Alvo para a imagem Registrada, que nada mais é que a imagem Alvo alinhada a imagem
Referência.

Podemos definir o processo de registro com a seguinte equação, como \cite{brown1992survey} fez em seu estudo:
\begin{align}\label{eq:defregistro}
    R(x,y) = g(A(f(x,y)))
\end{align}
    Representamos uma imagem como uma matriz de pixels e acessamos seus pixels utilizando a seguinte convenção $R(x,y)$ 
que nos dá o pixel da imagem $R$ na possição $(x,y)$. $f(x,y) = (x',y')$ é uma função que representa o deslocamento do 
campo vetorial encontrado pelo registro e $g$ é uma função que modifica a intensidade dos pixels, se for necessário. 
Cada algoritmo de registro utiliza um método diferente para encontrar a função de transformação $f$, mas os passos gerais são:
\begin{enumerate}
    \item Pré-processamento;
    \item Detecção de caracteristicas;
    \item Correspondência de caracteristicas;
    \item Estimativa da função de transformação;
    \item Reamostragem da imagem Alvo.
\end{enumerate}
    É importante salientar que nem todos algoritmos de registro seguem essa lista a risca, e mesmo que sigam eles ainda
podem realizar mais de um passo por vez. Os passos gerais são representados na imagem \ref{fig:regExplicacao}. 
Vamos tratar brevemente dos passos nas seções abaixo, explicitando em alguns pontos onde os passos podem ser acelerados,
principalmente pela paralelização de suas operações.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figuras/regSteps.jpg}
    \caption{Na primeira linha estam a imagem referência e a alvo, respectivamente. Na segunda linha as características
da imagem referência foram encontradas e suas correspondentes na imagem Alvo. A última linha mostra a estimação da 
função e a sua reamostragem da imagem Alvo. \citep{zitova2003image}}
    \label{fig:regExplicacao}
\end{figure}

%% ------------------------------------------------------------------------- %%
\subsection{Pré-processamento}
    Essa é a etapa mais aberta dentre todas, já que sua aplicação é totalmente dependente do problema a ser resolvido. 
Antes de iniciar o processo de registro, é possível que as imagens tenham que passar por algum processamento para 
melhorar o resultado final do registro. O pré-processamento muda de acordo com as necessídades de cada caso. Caso as
imagens tenham muito ruido, essa é a hora de aplicar filtros para melhorar sua qualidade, como um filtro Gaussiano ou 
o de Medianas. Certos algoritmos de registro não trabalham com as imagens, e sim com segmentações delas ou somente com 
suas bordas, então é necessário aplicar algoritmos como o \textit{Watershed} ou o \textit{Canny} para obter as 
segmentações e bordas, respectivamente.

%% ------------------------------------------------------------------------- %%
\subsection{Detecção de caracteristicas}\index{Detecção de caracteristicas}
\label{sec:dec_corr_carac}

    Com as imagens já pré-processadas, o primeiro passo para um algoritmo de registro é a localização de estruturas de 
destaque dentro da cena ou objeto representado pelas imagens. Caracteristicas devem ser facilmente identificadas, 
independente de variações na aquisição das fotografias, como mudanças na ângulação ou perspectiva. 
Elas são separadas em 3 grupos, baseando-se nas suas propriedades:

\textbf{Características de Região} - São caracteristicas encontradas utilizando a diferença de contraste entre elas e
as regiões vizinhas. Lagos, florestas ou regiões urbanas são exemplos desse tipo de característica.
Sua detecção é feita utilizando algoritmos de segmentação.

\textbf{Características de Retas} - Essas caracteristicas são marcadas pelas suas bordas. Podem ser ruas, rios ou o 
litoral. Métodos clássicos de detecção de bordas como o Canny ou o filtro laplaciano são usados para identificar essas 
características.

\textbf{Características de Ponto} - São pontos de intersecção entre linhas, representados por intersecções de ruas ou
rios, ou pontos de máxima curvatura. Algoritmos para identificação de pontos utilizam técnicas mais avançadas, dada a 
dificuldade de encontrá-los. Os mais básicos encontram as intersecções de linhas enquanto os mais avançados buscam
centróides de regiões ou o máximo local de uma \textit{wavelet}.

    Dada a importância desse passo, vários algoritmos foram desenvolvidos com o passar dos anos para resolver de maneira
rápida e eficiente a detecção de caracteristicas. Alguns dos mais famosos, como o 
\textit{Scale Invariant Feature Transform} (SIFT) , introduzido por \cite{lowe1999object}, transforma uma imagem em uma 
coleção de vetores de caracteristicas locais, que são usados para identidicar as caracteristicas. Outro algoritmo famoso,
desenvolvido por \cite{bay2006surf}, o \textit{Speeded Up Robust Features} (SURF) utiliza o determinante da matriz de
Hessian para identificar regiões da imagem que tenham um valor diferenciado para alguma propriedade, como brilho, de 
regiões vizinhas. Ao identificar uma região, ele cálcula a caracteristica dela utilizando a soma dos 
\textit{Wavelets de Haar}.

    A verificação de existencia de caracteristica em um pixel não interfere na verificação em quaisquer outro pixel da
imagem, já que não precisamos modificar a imagem em nenhuma maneira. Uma simples paralelização do processamento já o
acelera.

%% ------------------------------------------------------------------------- %%
\subsection{Correspondência de caracteristicas}\index{Correspondência de caracteristicas}

    Com as caracteristiscas de cada uma das imagens encontradas, o próximo passo é realizar a correspondencia entre elas.
A função desse passo é encontrar a correspondência entre pontos da imagem Referência para a imagem Alvo, ou vice-versa. 
O processo pode ser realizado tanto escolhendo ponto a ponto da imagem Referência e procurando o ponto com maior valor 
de proximidade entre os pontos da imagem Alvo, quanto utilizando métodos estatisticos para determinar quais pontos são 
correspondentes entre as duas imagens. É importante que esse passo consiga identificar pontos fisicos, com coordenadas, 
em cada par de caracteristicas, para que uma primeira estimativa dos parametros da função de transformação possa ser 
utilizada como ponto de partida para o próximo passo.

    A primeira solução a ser apresentada, a \textbf{Correspondência por Área}, mescla o passo de Detecção com o de 
Correspondência. Esse método utiliza duas janelas, uma em cada imagem, com formato retangular ou circular, aplicando
métricas em cada uma das janelas com a finalidade de calcular a relação entre as janelas. O algoritmo segue realizando
esse cálculo para todas as combinações possiveis de janelas entre as duas imagens, e sempre que um máximo é encontrado
o centro das janelas são usados para marcar a correspondência. Várias métricas podem ser utilizadas, como a 
Correlação entre as intensidades das janelas, o estudo do spectro da transformada de \textit{Fourier} das janelas ou 
o cálculo da informação mutua entre elas.

    A descoberta de correspondencia entre \textbf{Caracteristicas de Região} é feita utilizando, principalmente, duas 
técnicas. A primeira, mais simples na sua ideia, é parear regiões da imagem Referência com a região da imagem Alvo a qual
tenha o contorno mais parecido possível. Os métodos que fazem esse pareamento devem ser capazes de encontrar os pares mesmo
que eles tenham sofrido rotações ou mudança de escala. Descritores de Fourier e representações matriciais das regiões são
exemplos de métodos usados para realizar a correspondencia entre caracteristicas de região.

    Os metodos para encontrar correspondencias entre \textbf{Caracteristicas de Retas} tem as mesmas restrições que os
de Região, ou seja, devem conseguir realizar o pareamento de linhas que sofreram rotação, mudança de escala ou translação.
O primeiro passo de um algoritmo é comparar as retas da imagem Referência com todas as possiveis rotações de todas as retas
da imagem Alvo. Quando um possivel par é encontrado, um valor de correspondencia é calculado, utilizando um peso maior para
a direção da reta, algo que não sofre tanta influência de ruido, e um peso menor para atributos como comprimento e largura,
que são influenciados pelo ruido. O algoritmo ainda deve ser capaz de parear mais de duas retas por vez, dada a possibilidade
de uma reta em uma das imagens ser representada por duas retas na outra imagem.

    Por fim, as correspondencias entre \textbf{Caracteristicas de Ponto} podem ser encontradas utilizando-se várias técnicas
distintas. Como o número e o agrupamento de pontos geralmente não muda tanto com rotações e translações de imagens, métodos de 
\textit{Clustering} são usados para parear pontos. Os \textit{Clusters} são montados com base na proximidade dos parametros
de uma transformação que leve pontos da imagem Alvo para pontos da imagem Referência. Outro método define as caracteristicas
atraves de descritores invariantes que podem obedecer a 4 regras: \textbf{Invariancia}, caracteristicas correspondentes 
devem ter os mesmos descritores; \textbf{Unicidade}, caracteristicas diferentes devem ter descritores diferentes; 
\textbf{Estabilidade}, os descritores devem deformar de maneira proporcional a deformação aplicada na imagem; e 
\textbf{Independencia}, se o descritor for representado por um vetor, seus componentes devem ser independentes. 
O que define um descritor deve ser decidido caso a caso. O modelo mais simples usado é a propria intensidade do pixel, 
e a intensidade de seus vizinhos. Outros descritores válidos são ângulos entre correspondências vizinhas ou a distribuição
espacial dos vizinhos.

    Podemos separar os algoritmos de correspondencias em dois tipos com base no estilo com que eles fazem a busca de 
correspondencias. O primeiro tipo escolhe uma correspondencia na imagem Referência e realiza uma busca em largura por
todas as correspondencias da imagem Alvo, buscando aquela com maior relação com a da imagem Referência. Esse processo
pode ser fácilmente acelerado utilizando uma paralelização com todas as correspondencias da imagem Referência. Já o outro
tipo realiza uma busca entre as relações das caracteristicas e encontra os pares de maneira iterativa. A aceleração
desse passo depende totalmente da sua implementação.

    O modelo \textit{MapReduce} apresenta uma outra alternativa para acelerar esse passo e o anterior. Ele foi 
desenvolvido por \cite{dean2008mapreduce} e pela Google. O modelo \textit{MapReduce} foi desenvolvido para realizar 
processamento em lote de um número gigantesco de dados, o popular \textit{Big Data}, dado que não exista muita diferença 
em como os dados devem ser processados. Seu modelo de programação contém duas etapas principais, o \textit{Map}, que 
recebe um par de dados e retorna um par de valor/chave. Um passo intermediário agrupa todos os valores com mesma chave e
a lista resultante é enviada para o próximo passo, o \textit{Reduce}, onde a lista é processada. O nosso \textit{Map} 
recebe uma imagem Alvo e Referência, e encontra as caracteristicas delas, enquanto o processo de agrupamento encontra as
suas correspondencias. O \textit{Reduce} fica com qualquer etapa de pós-processamento, se necessário.
%% ------------------------------------------------------------------------- %%
\subsection{Estimativa da função de transformação}\index{Estimativa da função de transformação}
    
    Com o conjunto de correspondencias encontrado pelo passo anterior, os algoritmos de registro tem uma base para 
começar o processo de estimativa da função de transformação. Cada algoritmo assume um modelo de transformação diferente
para a deformação que a imagem sofreu, como por exemplo, uma modelagem elástica, por propagação de fluidos ou uma
simples translação. Juntando a modelagem com as correspondências, os algoritmos conseguem estimar parametros iniciais
para a função de transformação. Utilizando alguma métrica para passear pelo espaço de parametros, os algoritmos encontram
algum conjunto de parametros que alinhe a imagem Alvo com a imagem Referência. Na seção \ref{sec:algReg}, apresentaremos
dois algoritmos estudados com enfasse nessa etapa.

%% ------------------------------------------------------------------------- %%
\subsection{Reamostragem da imagem Alvo}\index{Reamostragem da imagem Alvo}

    O último passo do registro é a montagem da imagem Registrada, ou a reamostragem da imagem Alvo. O passo anterior
dá um conjunto de parametros para a montagem da imagem final, logo esse passo tem como objetivo a aplicação
da transformação $f$ utilizando os parametros encontrados sob todas as posições da imagem Alvo. Temos:

\begin{align}\label{eq:reamostragem}
    F(x_i,y_j) = A(f_o(x_i,y_j))), \forall (i = 1, \dots, n_c), (j = 1, \dots, n_l)
\end{align}

    Onde $F(x_i,y_j)$ representa a posição $(x_i,y_j)$ da imagem Registrada e $f_o$ é a função $f$ sob os parametros
encontrados no passo anterior.

%% ------------------------------------------------------------------------- %%
%% ------------------------------------------------------------------------- %%
\section{Computação de Alto Desempenho}\index{HPC,GPGPU}\label{GPGPU}
    
    Computação de Alto Desempenho (\textit{High Performance Computing} - HPC) são sistemas de alta capacidade de processamento
e armazenamento de dados montados para resolver grandes problemas cientificos, para os quais um computador pessoal não
é o suficiente. Esses sistemas variam em tamanho, poder computacional e capacidade de armazenamento. Os mais famosos,
conhecido como Supercomputadores, são máquinas montadas com a finalidade de resolver um único problema, ou um grupo
especifico de problemas, e tem em sua composição milhares de processadores. Porém existem instâncias mais simples de 
\textit{HPC}, onde um sistema é montado a partir de um \textit{Cluster} de computadores pessoais. Uma arquitetura \textit{HPC}
pode ter como principal hardware de processamento as Unidades de Processamento Gráfico (\textit{Graphic Processing Units} - GPU).

\subsection{Unidade de Processamento Gráfico}
    A GPU nasceu da necessidade dos antigos \"arcades\" de renderizar cenas complexas mantendo uma taxa de quadros por 
segundo aceitavel para o jogador. Para cumprir com essa necessidade, um hardware foi desenvolvido para seguir uma
sequência fixa de passos que levam os dados da cena até uma renderização dela na tela em questão de milisegundos.
Essa sequência de passos é chamada de \textit{Pipeline}, e se assemelha a uma linha de montagem de fabricas, onde
as peças são montadas uma a uma e em linha. Esse \textit{Pipeline} evoluiu com a criação de técnicas melhores e da criação
de hardwares mais especificos. No inicio dos anos 2000, com o inicio do suporte de operações de ponto flutuante, ainda que
elas fossem emuladas, e com o surgimento de um \textit{shaders} programável, a GPU começou a ser usada para resolver
cálculos matemáticos. Chamamos a aplicação de GPUs na solução de problemas computacionais, fora da área de computação 
gráfica, de \textit{General-purpose computing on graphics processing units}, ou GPGPU. 

\subsection{Arquitetura de uma GPU}

    A GPU 
A placa contém um escalonador para threads implementado em hardware. Ele é responsável por escalonar as threads que serão
executadas nos streaming multiprocessors (SM). Um SM é um conjunto de 48 processadores, um pequeno bloco de memória própria,
um cache de instruções e 8 unidades de funções gráficas. A Geforce GTX 760 tem 7 SMs, totalizando 336 processadores.

O código que será executado em cada processador é chamado de \textbf{kernel}. Ao executar um kernel na GPU, o 
hardware criará threads, cada uma delas executando o mesmo código, mas com dados diferentes. Nas placas NVIDIA as threads 
são agrupadas em blocos, e esses blocos são escalonados para cada SM. Depois, todas as threads dentro de um bloco são 
divididas em pequenos grupos chamados de \textbf{warp}, e cada warp é executado paralelamente dentro do 
mesmo SM para qual o bloco foi escalonado. Existe um limite para a quantidade de threads escalonadas para execução
dentro de um SM, que é definida pelos recursos que cada thread consome. Por exemplo, não há como executar 10 threads
que consomem 10 registradores cada em um SM com 90 registradores.

Outra parte importante do hardware é a memória, que é limitada em relação à da CPU. GPUs tem, em média, 1GB
de memória, enquanto CPUs tem 4GB. O acesso a um mesmo bloco de memória é concorrente, mas ao utilizar caches e leitura ou escritas em
conjunto podemos minimizar a taxa com que leituras ou escritas conflitantes são feitas. Mas ainda sim é necessário atenção ao escrever um
kernel. Dada a estrutura do hardware da GPU, é melhor deixar threads que façam operações sobre posições de memória próximas no mesmo
SM, assim elas podem utilizar a memória compartilhada do mesmo, e elas podem requisitar em conjunto um mesmo bloco da memória principal,
se necessário.

No caso da GTX 760 cada SM tem um bloco de memória de 64KB. Esse bloco pode ser configurado para 16KB de memória compartilhada e 48KB
de cache L1 ou vice versa. A memória principal da placa é de 1024MB com conexões de 256 bits. A placa também tem um
cache L2 de 512KB.

Outro fator limitante é a transferência de dados da memória principal do computador para a memória 
principal da GPU. A transmissão é feita por um barramento PCI Express, com velocidades de até 16GB/s ( dado que o
barramento seja utilizado somente pela GPU ). Essa transmissão é a parte mais lenta de todo o
processo de execução na GPU e dado isso, em alguns casos é mais viável executar na GPU um pedaço do seu programa que seria executado
na CPU do que retornar os dados computados na GPU para a CPU, executar esse pedaço especifico, e passá-los de volta para a GPU 
para mais operações e novamente retornar esses dados para a CPU no final, passando duas vezes a mais pelo PCI Express. 

Ao estudar como o código é executado nas GPUs NVIDIA descobrimos a existência de uma máquina virtual chamada de Parallel Thread Execution.
Todo kernel é primeiro compilado para um arquivo .ptx que é executado na GPU através da máquina PTX. Ela é utilizada para garantir 
a retrocompatibilidade de kernels em placas mais antigas.
\subsection{Single Instruction Multiple Data}

\subsection{CUDA}
\textit{Compute Unified Device Architecture}, definida pela (CUDA) é uma arquitetura de programação para GPUs criada 
pela ~\cite{nvidia2007compute}.
Ele adiciona suas diretrizes para as linguagens C, C++, FORTRAN e Java, permitindo que elas usem a GPU.
Esse trabalho usa o CUDA junto com a linguagem C.
A versão 1.0 do CUDA foi disponibilizada no inicio de 2007. Atualmente só existe um compilador para CUDA, o nvcc,
e ele só da suporte para GPUs NVIDIA.

Para uma função executar na GPU ela precisa ser invocada de um programa da CPU. Chamamos esse programa de \textit{Host}
e a GPU onde o kernel irá executar de \textit{Device}.

O CUDA implementa um conjunto virtual de instruções e memória, tornando os programas retroativos. O compilador
primeiro compila o código em C para um intermediário, chamado de PTX, que depois será convertido em linguagem
de máquina. Na conversão do PTX para linguagem de máquina o compilador verifica quais instruções o \textit{device}
suporta e converte o código para usar as instruções corretas.
Para obter o maior desempenho possível, é importante saber para qual versão o código final será compilado, 
pois na passagem do código de uma versão maior para uma menor não existe a garantia que o algoritmo seguira as mesmas instruções, 
o compilador pode mudar um conjunto de instruções para outro menos eficiênte, ou em alguns casos, algumas instruções não existem em
versões mais antigas do hardware.

\subsubsection{Modelo de Plataforma}
A inicialização dos recursos que o CUDA necessita para a comunicação com a GPU é feita no background da
aplicação no momento da primeira chamada de alguma das diretivas do CUDA. Essa primeira diretiva terá um
tempo maior de execução que chamadas subsequentes a mesma diretiva. Na inicialização o CUDA identifica
os \textit{devices} existentes e escolhe um deles para ser o responsável pelas execuções posteriores.

O próximo passo é a alocação de memória no \textit{device}. As operações de leitura de memória de um kernel são feitas somente
na memória de um \textit{device}. A alocação dessa memória é feita pelo \textit{host}, usando \verb#cudaMalloc()#. 
Para copiar a memória do \textit{host} para o \textit{device} ou vice-versa,
\verb#cudaMemcpy()# é usada. Para liberar o espaço alocado após a execução basta usar o \verb#cudaFree()#.
Todas essas diretivas recebem um ponteiro do \textit{host}, usado para o controle sobre qual posição da memória está sendo
operado em cada operação.

O CUDA dá suporte a alocação de vetores em duas ou três dimensões através de: \verb#cudaMallocPitch()# e 
\verb#cudaMalloc3D()#, respectivamente. É necessário usar as modificações dos comandos \verb#Memcpy# para
duas ou três dimensões também, que são: \verb#cudaMemcpy2D()#, \verb#cudaMemcpy3D()#.

\subsubsection{Modelo de Programação}
Um kernel no CUDA é uma função C que será executada paralelamente $n$ vezes em $n$ threads diferentes na GPU. Um kernel pode ser
definido em qualquer lugar do seu código, usando a declaração \verb#__global__# do lado esquerdo do tipo de retorno do kernel.
Para invocar um kernel, o \textit{host} faz a chamada de uma função com a sintaxe parecida com o C, mas usa uma configuração de
execução definida pelo CUDA, que usa a sintaxe \verb#<<<...>>># junto da chamada da função. Os parâmetros da configuração são
o número de blocos de threads e o número de threads por blocos. Para somar dois vetores de tamanho M e guardar o resultado num
outro vetor, o código é o seguinte:

\begin{lstlisting}
  __global__ void MatrixMulti ( float* a, float* b, float* c) { 
    int i = threadIdx.x;
    a[i] = b[i] + c[i];        
  }
                            
  int main () {               
    ...                       
    VecAdd<<<1,M>>>(a, b, c)  
    ...                       
  }                                 
\end{lstlisting}

No kernel acima, a linha \verb#int i = threadIdx.x# atribui a variável i o valor do indice da thread atual na primeira dimensão. 
A estrutura \verb#threadIdx# é um vetor de 3 dimensões, logo as threads podem ser organizadas em 1, 2 ou 3 dimensões dentro de um
\textit{device}. As threads são organizadas por blocos. Cada bloco tem dimensões maleáveis, mas as GPUs atuais limitam para 1024 o 
número máximo de threads por blocos. Cada bloco é lançado para execução em um processador diferente. Blocos são organizados em 
grids, que tem seu tamanho configurado na chamada o kernel, bem como o tamanho de cada bloco. No nosso exemplo acima, na linha
\verb#VecAdd<<<1,M>>>(a,b,c)#, o 1 determina o número de blocos e o M o número de threads por bloco.

O CUDA supõem que todos os blocos podem ser executados de maneira independende, ou seja, eles podem executar tanto paralelamente
quanto sequencialmente. Com isso, é possivel que o desempenho do código aumente em GPUs com mais processadores, sem que o programador
tenha que modificar o código.

O CUDA sabe qual instruções ele pode executar dentro de um \textit{device} baseando-se no seu Compute Capability 
(Capacidade Computacional). A Compute Capability de um \textit{device} são dois números, um que representa a arquitetura do 
\textit{device}, e outro que representa melhorias numa arquitetura.
A arquitetura \textit{Tesla}, a primeira da NVIDIA a dar suporte a GPGPU, tem Compute Capability 1.x, a seguinte, a \textit{Tesla},
tem 2.x e a atual, a \textit{Kepler}, tem 3.x. Dentro de cada arquitetura, podem existir melhorias nas instruções, que são
refletidas no número após o ponto, ou seja, uma placa com Compute Capability 2.1 tem instruções que uma 2.0 não tem.

\subsubsection{Hierarquia de Memória}
No CUDA, a memoria é separada lógicamente em 4 locais:

\begin{itemize}
  \item Registradores - Toda variável de uma thread fica em registradores.
  \item Memória Local - Memória acessivel por cada thread separadamente, mas de uso pouco provável. Ela só é usada se
          não existe mais espaço nos registradores ou se o compilador não ter certeza sobre o tamanho de um vetor.
  \item Memória Compartilhada - Cada bloco de threads tem uma memória compratilhada. A memória compartilhada é separada em
          pequenos blocos independentes. Se uma requisição de leitura tem n endereços em n blocos diferentes, o tempo de leitura
          desses n endereços é igual ao tempo de leitura de 1 endereço. Caso duas leituras caiam no mesmo bloco, elas serão
          serializadas. A memória compatilhada fica em chips dentro dos SMs, logo seu acesso é mais rápido do que o acesso a
          memória global.
  \item Memória Global - A memória global é acessivel por qualquer bloco em execução em um \textit{device}. A memoria global não é
          resetada após a execução de um kernel, então chamadas subsequentes de um mesmo kernel simplesmente leêm os resultados
          da memória global. Existe um pedaço da memória global reservada para valores constantes do programa.
\end{itemize}

Por padrão, o compilador do CUDA cuida do gerenciamento da memória, ou seja, ele é o responsável por distribuir os dados 
entre os locais diferentes de memória. O programador pode dar dicas para o compilador usando qualificadores indicando o local
que ele quer que aquele elemento fique na memória. Os possiveis qualificadores são:
\begin{itemize}
  \item \verb#__device__# Fica na memória global.
  \item \verb#__constant__#   Fica na area constante da memória global.
  \item \verb#__shared__# Fica na memória compartilhada das threads.
  \item \verb#__restrict__# Indica para o compilador que todos os ponteiros com esse qualificador apontam para locais diferentes
                            da memória. Isso é importante pois o compilador pode fazer otimizações com o código sabendo dessa informação.   
\end{itemize}

GPUs com Compute Cabapility maior ou igual a 2.0 podem alocar memória dentro do \textit{device} em tempo de execução.

%% ------------------------------------------------------------------------- %%
%% ------------------------------------------------------------------------- %%
\section{Algoritmos de Registro}\label{sec:algReg}